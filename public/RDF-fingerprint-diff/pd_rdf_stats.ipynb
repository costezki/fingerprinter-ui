{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint as pprint\n",
    "\n",
    "def read_prefixes(filename):\n",
    "    \"\"\" \n",
    "    :param filename: path to a CSV file with columns \"ns\", \"uri\"\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    return pd.read_csv(filename, header=0, names=[\"ns\", \"uri\"])\n",
    "\n",
    "def read_fp_distinct(filename):\n",
    "    \"\"\" \n",
    "    :param filename: path to a CSV file with columns \"s\", \"p\", \"o\"\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    return pd.read_csv(filename, header=0, names=[\"s\", \"p\", \"o\"])\n",
    "\n",
    "def read_fp_count(filename):\n",
    "    \"\"\" \n",
    "    :param filename: path to a CSV file with columns \"s\", \"p\", \"o\", \"c\"\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    return pd.read_csv(filename, header=0, names=[\"s\", \"p\", \"o\", \"c\"])\n",
    "\n",
    "def replace_ns(triples, ns_dataframe):\n",
    "    \"\"\" \n",
    "    :param triples: a dataframe with columns (s,p,o)\n",
    "    :param ns_dataframe: a dataframe with columns (ns,uri)\n",
    "    :return: triples replaced with ns\n",
    "    \"\"\"\n",
    "    d = dict(zip(ns_dataframe.uri, ns_dataframe.ns))\n",
    "    return triples.replace(d, regex=True)\n",
    "\n",
    "def df_to_set_of_tuples(df, list_of_desired_columns=[\"s\", \"p\", \"o\"]):\n",
    "    \"\"\" \n",
    "    :param df: the fingerprint dataframe \n",
    "    :param list_of_desired_columns: in case it is a fingerprint with counts then this is an option \n",
    "            to remove the count column, or select any combination of columns\n",
    "    :return: set of tuples\n",
    "ns = read_prefixes(\"test/prefix.csv\")\n",
    "\n",
    "fp1 = replace_ns(fp1,ns)\n",
    "fp2 = replace_ns(fp2,ns)\n",
    "\n",
    "print \"Done.\"\n",
    "    \"\"\"\n",
    "    return set([tuple(line) for line in df[list_of_desired_columns].values.tolist()])\n",
    "\n",
    "def df_diff(alpha, beta):\n",
    "    \"\"\" \n",
    "    provides the set difference between two dataframes\n",
    "    :param alpha: first dataframe\n",
    "    :param beta: second dataframe\n",
    "    :return: (a^b, a - b, b - a)\n",
    "    \"\"\"\n",
    "    a = df_to_set_of_tuples(alpha)\n",
    "    b = df_to_set_of_tuples(beta)\n",
    "    return a.intersection(b), a.difference(b), b.difference(a)\n",
    "\n",
    "\n",
    "fp1 = read_fp_count(\"test/dataset_figerprint_for_count.rq_eurovoc44.log\")\n",
    "fp2 = read_fp_count(\"test/dataset_figerprint_for_count.rq_EV45OLD.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section defines functions generating parts of latex documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatex import Package\n",
    "from pylatex import Section, Subsection, Document\n",
    "from pylatex.base_classes import Environment\n",
    "from pylatex.utils import NoEscape\n",
    "\n",
    "\n",
    "class LandscapeEnvironment(Environment):\n",
    "    _latex_name = 'landscape'\n",
    "    packages = [Package('pdflscape')]\n",
    "\n",
    "def diff_to_latex_section(tex_doc, alpha, alpha_description, beta, beta_description,\n",
    "                          cols=[\"Subject\", \"Predicate\", \"Object\"]):\n",
    "    \"\"\" \n",
    "    :param tex_doc: the pylatex document\n",
    "    :param alpha: the first fingerprint\n",
    "    :param alpha_description: the first fingerprint description\n",
    "    :param beta: the second fingerprint\n",
    "    :param beta_description: the second fingerprint description\n",
    "    :return: returns a section of latex document with deltas\n",
    "    \"\"\"\n",
    "    cmn_s, adb_s, bda_s = df_diff(alpha, beta)\n",
    "\n",
    "    cmn_df = pd.DataFrame(list(cmn_s), columns=cols, )\n",
    "    cmn_df.sort_values(by=cols, inplace=True)\n",
    "\n",
    "    adb_df = pd.DataFrame(list(adb_s), columns=cols, )\n",
    "    adb_df.sort_values(by=cols, inplace=True)\n",
    "\n",
    "    bda_df = pd.DataFrame(list(bda_s), columns=cols, )\n",
    "    bda_df.sort_values(by=cols, inplace=True)\n",
    "\n",
    "    ref_alpha = alpha_description[\"title\"]\n",
    "    ref_beta = beta_description[\"title\"]\n",
    "\n",
    "    section_title = 'Difference between ' + ref_alpha + ' and ' + ref_beta\n",
    "    with tex_doc.create(LandscapeEnvironment()):\n",
    "        with tex_doc.create(Section(section_title)):\n",
    "            # tex_doc.append(alpha_description)\n",
    "            # tex_doc.append(beta_description)\n",
    "            with tex_doc.create(Subsection(\"Common parts\")) as subsec:\n",
    "                subsec.append(\"The table below represents the elements common to both datasets.\")\n",
    "                # with tex_doc.create(Table(position='H')) as tbl:\n",
    "                tex_doc.append(NoEscape(cmn_df.to_latex(longtable=True, index=False)))\n",
    "\n",
    "            with tex_doc.create(Subsection(\"Unique to \" + ref_alpha)) as subsec:\n",
    "                subsec.append(\n",
    "                    \"The table below represents the elements present in \" + ref_alpha + \" but missing in \" + ref_beta + \".\")\n",
    "                # with tex_doc.create(Table(position='H')) as tbl:\n",
    "                tex_doc.append(NoEscape(adb_df.to_latex(longtable=True, index=False)))\n",
    "\n",
    "            with tex_doc.create(Subsection(\"Unique to \" + ref_beta)) as subsec:\n",
    "                subsec.append(\n",
    "                    \"The table below represents the elements present in \" + ref_beta + \" but missing in \" + ref_alpha + \".\")\n",
    "                # with tex_doc.create(Table(position='H')) as tbl:\n",
    "                tex_doc.append(NoEscape(bda_df.to_latex(longtable=True, index=False)))\n",
    "\n",
    "                # diff_to_latex_table(fp1, \"Bla Bla\", fp2, \"Blu blu\")\n",
    "\n",
    "\n",
    "configuration_dict = {\n",
    "    \"type\": \"difference between two dataset fingerprints\",\n",
    "    \"alpha\": {\"title\": \"EuroVoc 4.4\",\n",
    "              \"filename\": \"test/dataset_figerprint_for_count.rq_eurovoc44.log\",\n",
    "              \"desc\": \"EuroVoc 4.4 was released a long time ago using EuroVoc Ontology\"},\n",
    "    \"beta\": {\"title\": \"EuroVoc 4.5\",\n",
    "             \"filename\": \"test/dataset_figerprint_for_count.rq_EV45OLD.log\",\n",
    "             \"desc\": \"EuroVoc 4.5 was released in July 2016 with SKOS-AP-EU and then converted to fit also the old EuroVoc Ontology.\"},\n",
    "}\n",
    "\n",
    "def generate_document(filename, config=configuration_dict):\n",
    "    \"\"\" \n",
    "    :param filename: filename for the tex document\n",
    "    :param config: \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    geometry_options = {\n",
    "        \"head\": \"40pt\",\n",
    "        \"margin\": \"0.5in\",\n",
    "        \"bottom\": \"0.6in\",\n",
    "        \"includeheadfoot\": True\n",
    "    }\n",
    "    doc = Document('basic', geometry_options=geometry_options)\n",
    "    \n",
    "    doc.packages.append(Package('longtable'))\n",
    "    doc.packages.append(Package('booktabs'))\n",
    "    doc.packages.append(Package('float'))\n",
    "    doc.packages.append(Package('ltablex'))\n",
    "    doc.packages.append(Package('pdflscape'))\n",
    "    \n",
    "    first_page = PageStyle(\"firstpage\")\n",
    "    \n",
    "    \n",
    "    # Add document title\n",
    "    with first_page.create(Head(\"R\")) as right_header:\n",
    "        with right_header.create(MiniPage(width=NoEscape(r\"0.49\\textwidth\"),\n",
    "                                 pos='c', align='r')) as title_wrapper:\n",
    "            title_wrapper.append(LargeText(bold(\"Bank Account Statement\")))\n",
    "            title_wrapper.append(LineBreak())\n",
    "            title_wrapper.append(MediumText(bold(\"Date\")))\n",
    "\n",
    "    # Add footer\n",
    "    with first_page.create(Foot(\"C\")) as footer:\n",
    "        message = \"Important message please read\"\n",
    "        with footer.create(Tabularx(\n",
    "                \"X X X X\",\n",
    "                width_argument=NoEscape(r\"\\textwidth\"))) as footer_table:\n",
    "\n",
    "            footer_table.add_row(\n",
    "                [MultiColumn(4, align='l', data=TextColor(\"blue\", message))])\n",
    "            footer_table.add_hline(color=\"blue\")\n",
    "            footer_table.add_empty_row()\n",
    "\n",
    "            branch_address = MiniPage(\n",
    "                width=NoEscape(r\"0.25\\textwidth\"),\n",
    "                pos='t')\n",
    "            branch_address.append(\"960 - 22nd street east\")\n",
    "            branch_address.append(\"\\n\")\n",
    "            branch_address.append(\"Saskatoon, SK\")\n",
    "\n",
    "            document_details = MiniPage(width=NoEscape(r\"0.25\\textwidth\"),\n",
    "                                        pos='t', align='r')\n",
    "            document_details.append(\"1000\")\n",
    "            document_details.append(LineBreak())\n",
    "            document_details.append(simple_page_number())\n",
    "\n",
    "            footer_table.add_row([branch_address, branch_address,\n",
    "                                  branch_address, document_details])\n",
    "\n",
    "    doc.preamble.append(first_page)\n",
    "    # End first page style\n",
    "    \n",
    "\n",
    "\n",
    "    diff_to_latex_section(doc, fp1, config[\"alpha\"], fp2, config[\"beta\"])\n",
    "    doc.generate_tex(filepath=filename)\n",
    "    doc.generate_pdf(clean_tex=False, filepath=filename)\n",
    "\n",
    "generate_document(\"temp/diff_report\", configuration_dict)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "testing class statistics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Unique instances</th>\n",
       "      <th>% from scaled total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>skoxl:Label</td>\n",
       "      <td>41099</td>\n",
       "      <td>80.709713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>euvoc:XlNotation</td>\n",
       "      <td>3244</td>\n",
       "      <td>6.370527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ato:MappedCode</td>\n",
       "      <td>1340</td>\n",
       "      <td>2.631476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>skos:Concept</td>\n",
       "      <td>1176</td>\n",
       "      <td>2.309414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>dct:Agent</td>\n",
       "      <td>1139</td>\n",
       "      <td>2.236754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>org:Organisation</td>\n",
       "      <td>1139</td>\n",
       "      <td>2.236754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>foaf:Agent</td>\n",
       "      <td>1139</td>\n",
       "      <td>2.236754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>euvoc:XlNote</td>\n",
       "      <td>618</td>\n",
       "      <td>1.213621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>euvoc:LabelType</td>\n",
       "      <td>11</td>\n",
       "      <td>0.021602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>euvoc:ConceptStatus</td>\n",
       "      <td>9</td>\n",
       "      <td>0.017674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>owl:Ontology</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>skos:ConceptScheme</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "from df_io import read_prefixes, read_fp_spo_count, replace_ns\n",
    "\n",
    "ns = read_prefixes(\"test/prefix.csv\")\n",
    "\n",
    "df = replace_ns(read_fp_spo_count(\"test/test_fingerprint_spo.csv\"), ns)\n",
    "\n",
    "df = df[df['p'] == 'rdf:type'][['stype', 'scnt']].sort_values(by='scnt', ascending=False)\n",
    "\n",
    "total_scnt = df['scnt'].sum()\n",
    "df['rel_stype_scnt'] = df['scnt'] / total_scnt * 100\n",
    "\n",
    "print df['rel_stype_scnt'].sum()\n",
    "df.rename(columns={'stype': 'Class', 'scnt': 'Unique instances', 'rel_stype_scnt': '% from scaled total'}, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "testing prop statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:36: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:55: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:56: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:58: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-18655a414707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0munique_uris\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[0mgenerate_missing_ns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-18655a414707>\u001b[0m in \u001b[0;36mgenerate_missing_ns\u001b[1;34m(df, structural_columns)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0munique_uris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstructural_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0muris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_local_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0murl_local_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0munique_uris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_uris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muris\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-18655a414707>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((i,))\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0munique_uris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstructural_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0muris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_local_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0murl_local_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0munique_uris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_uris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muris\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-18655a414707>\u001b[0m in \u001b[0;36murl_local_split\u001b[1;34m(url_string)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mthat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mwhole\u001b[0m \u001b[0mstring\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlocal\u001b[0m \u001b[0msegment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \"\"\"\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[/#]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0murl_string\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_string\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36mfinditer\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         Empty matches are included in the result.\"\"\"\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "from df_desc_stats import *\n",
    "from df_io import read_prefixes, read_fp_spo_count, replace_ns\n",
    "import re\n",
    "\n",
    "ns = read_prefixes(\"resources/prefix.csv\")\n",
    "\n",
    "df = replace_ns(read_fp_spo_count(\"resources/test_fingerprint_spo.csv\"), ns)\n",
    "\n",
    "\"\"\"\n",
    " for each group of ['stype','p'] aim at reducing duplicates to one record bu following method.\n",
    " If the 'propType' is 'data' then sum the 'scnt', 'ocnt', 'cnt' and average the 'min_sp', \n",
    " 'max_sp' and 'avg_sp'.  If the 'propType' is 'object' then expect the numbers to be exactly the \n",
    " same on every row and then just copy the values of the first row.\n",
    "\"\"\"\n",
    "\n",
    "df_reduced = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# reducing the groups to single row groups\n",
    "# iterate over the groups of ['stype', 'p']\n",
    "for name, group in df.groupby(['stype', 'p']):\n",
    "    if len(group) > 1:\n",
    "        if len(group[group['propType'] == 'data']['propType']) > 0:\n",
    "            # process data rows\n",
    "            aggregated = {'stype': name[0], 'p': name[1], 'propType': 'data', 'ootype': '*'}\n",
    "            aggregated['scnt'] = [group['scnt'].sum()]\n",
    "            aggregated['ocnt'] = [group['ocnt'].sum()]\n",
    "            aggregated['cnt'] = [group['cnt'].sum()]\n",
    "            aggregated['min_sp'] = [group['min_sp'].min()]\n",
    "            aggregated['max_sp'] = [group['max_sp'].max()]\n",
    "            aggregated['avg_sp'] = [group['avg_sp'].mean()]\n",
    "            aggregated['ootype'] = \", \".join(group['ootype'])\n",
    "            df_reduced = df_reduced.append(pd.DataFrame(aggregated, columns=df.columns))\n",
    "        elif len(group[group['propType'] == 'object']['propType']) > 0:\n",
    "            # process object rows\n",
    "            g = group[0:1]\n",
    "            g['ootype'].iloc[0] = \", \".join(group['ootype'])\n",
    "            df_reduced = df_reduced.append(g)\n",
    "        else:\n",
    "            # serios offense this palce should never be reached\n",
    "            raise Exception(\"Is the query distinguishing more then two types of properties?\")\n",
    "    else:\n",
    "        # simple group of single row\n",
    "        df_reduced = df_reduced.append(group)\n",
    "\n",
    "# Next step\n",
    "df_class_st = df_class_stats(df)\n",
    "df_stats = None\n",
    "\n",
    "card = lambda x: str(x['min_sp']) + \" .. \" + str(x['max_sp']) + \"(\" + str(x['avg_sp']) + \")\"\n",
    "# calculating the averages and relatives per class for each property\n",
    "# iterate over the groups of ['stype']\n",
    "for name, group in df.groupby(['stype']):\n",
    "    type_scnt = group[group['p'] == 'rdf:type']['scnt'].iloc[0]\n",
    "    type_cnt = group[group['p'] == 'rdf:type']['cnt'].iloc[0]\n",
    "    group[\"scnt/type-scnt\"] = group['scnt'] / type_scnt * 100\n",
    "    group[\"cnt/type-cnt\"] = group['cnt'] / type_cnt * 100\n",
    "    group[\"caard\"] = group['min_sp'].astype(str) + \" .. \" + group['max_sp'].round().astype(str) + \"(\" + group[\n",
    "        'avg_sp'].astype(int).astype(str) + \")\"\n",
    "    if df_stats is None:\n",
    "        df_stats = pd.DataFrame(columns=group.columns)\n",
    "    df_stats = df_stats.append(group)\n",
    "\n",
    "\n",
    "df_stats['min_ap'] = df_stats[ df_stats['scnt/type-scnt'] > 80 ]['min_sp'].astype(int).astype(str)\n",
    "df_stats['min_ap'].fillna(0, inplace=True)\n",
    "\n",
    "df_stats['max_ap'] = df_stats[ (df_stats['scnt/type-scnt'] > 80) & (df_stats['max_sp'] ==1)]['max_sp'].astype(int).astype(str)\n",
    "df_stats['max_ap'].fillna(\"*\", inplace=True)\n",
    "\n",
    "df_stats[['stype','p','scnt','cnt','min_ap','max_ap']]\n",
    "\n",
    "\n",
    "def url_local_split(url_string):\n",
    "    \"\"\"\n",
    "    splits an url into base and local strings where the local is the lat segment after a # or /\n",
    "    :param url_string:\n",
    "    :return: a tuple of base and local string, if the delimiter char is not found returns (None,url_string) meaning\n",
    "    that the whole string is a local segment\n",
    "    \"\"\"\n",
    "    l = [i.start() for i in re.finditer(\"[/#]\", url_string)]\n",
    "    if l:\n",
    "        return url_string[:l[-1]+1], url_string[l[-1] + 1:]\n",
    "    return \"\", url_string\n",
    "\n",
    "\n",
    "def generate_missing_ns(df, structural_columns=['stype', 'p', 'ootype']):\n",
    "    \"\"\"\n",
    "    given a dataframe detect unique namespaces\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    unique_uris = set()\n",
    "    for col in structural_columns:\n",
    "        uris = set(url_local_split(i)[0] for i in df[col] if url_local_split(i)[0])\n",
    "        unique_uris = unique_uris.union(uris)\n",
    "        \n",
    "    print unique_uris\n",
    "\n",
    "generate_missing_ns(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('werwefds/#', '4333')"
      ]
     },
     "execution_count": 43,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}